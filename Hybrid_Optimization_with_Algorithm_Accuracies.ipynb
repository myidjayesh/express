{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myidjayesh/express/blob/master/Hybrid_Optimization_with_Algorithm_Accuracies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Install Required Packages\n",
        "!pip install nltk scikit-learn pandas indic-nlp-library\n",
        "\n",
        "#  Imports\n",
        "import numpy as np  # math function\n",
        "import pandas as pd  # table format\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Text to binary number\n",
        "from sklearn.model_selection import StratifiedShuffleSplit  # split into training and testing\n",
        "from sklearn.metrics import accuracy_score, classification_report  #\n",
        "from sklearn.naive_bayes import MultinomialNB  # EMotion Guessing algo\n",
        "from sklearn.utils import resample\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "\n",
        "#  Load Dataset\n",
        "data = pd.read_csv('hindi_emotion_dataset261.csv')\n",
        "\n",
        "#  Filter classes with <2 samples\n",
        "emotion_counts = data['emotion'].value_counts()  # Count emotion in dataset\n",
        "valid_classes = emotion_counts[emotion_counts >= 2].index  #\n",
        "data = data[data['emotion'].isin(valid_classes)]  # filters the dataset to keep only those emotion classes that appear at least twice, removing rare classes that occur only once.\n",
        "\n",
        "#  Hindi Stopwords\n",
        "stop_words = set([\n",
        "    'के', 'का', 'की', 'से', 'को', 'में', 'पर', 'और', 'भी', 'है', 'यह', 'थे', 'था', 'हूं', 'हो', 'गया', 'रहा', 'एक', 'तो', 'नीचे', 'मैं', 'मुझे', 'मेरा', 'स्वयं', 'हम', 'हमारा', 'हमारा ', 'वह', 'उसका', 'उसका', 'स्वयं', 'वह', 'उसकी', 'उसका', 'स्वयं ', 'यह', \"यह है\", 'इसका', 'स्वयं', 'वे', 'उन्हें', 'उनका', 'उनका',\n",
        "    'स्वयं', 'क्या', 'जो', 'कौन', 'जिसे', 'यह', 'वह', 'ये', 'वे', 'हैं', 'हैं', 'लिया', 'था', 'थे', 'होना', 'किया जाना', 'है', 'है', 'था', 'होने', 'करना', 'करता है', 'किया', 'कर रहा है', '!',\n",
        "    'एक', 'एक', 'वह', 'और', 'लेकिन', 'अगर', 'या', 'क्योंकि', 'जैसा', 'जब तक', 'जबकि', 'का', 'पर', 'द्वारा', 'के लिए', 'के साथ', 'के बारे में', 'खिलाफ', 'बीच में', 'में', 'के माध्यम से', 'दौरान', 'पहले', 'बाद में', 'ऊपर', 'नीचे', 'को',\n",
        "    'से', 'ऊपर', 'नीचे', 'अंदर', 'बाहर', 'चालू', 'बंद', 'ऊपर', 'नीचे', 'फिर', 'आगे', 'फिर', 'एक बार', 'यहाँ', 'वहाँ', 'कब', 'कहाँ', 'क्यों', 'कैसे', 'सभी', 'कोई भी', 'दोनों', 'प्रत्येक', 'कुछ', 'अधिक', 'अधिकतर',\n",
        "    'अन्य', 'कुछ', 'ऐसा', 'नहीं', 'न ही', 'नहीं', 'केवल', 'स्वयं का', 'वही', 'इसलिए', 'से', 'भी', 'बहुत', 'हूं', 'एस', 'टी', 'कर सकते हैं', 'करेंगे', 'बस', 'रहा', 'नहीं करें', 'करना चाहिए', 'होना चाहिए था', 'अब', 'डी', 'करूंगा', 'एम', 'ओ', 'री', 'वे', 'वाई', 'ऐन', 'नहीं हैं', 'नहीं कर सके', 'नहीं कर सके', 'नहीं किया', 'नहीं किया', 'नहीं करता', 'नहीं करता', 'नहीं था', \"नहीं है\", 'माँ', 'हो सकता है',\n",
        "    \"हो सकता है नहीं\", 'चाहिए', \"नहीं चाहिए\", 'ज़रूरत नहीं', \"ज़रूरत नहीं\", 'शान', \"नहीं करना चाहिए\", 'नहीं करना चाहिए', 'नहीं था', 'नहीं था', 'नहीं थे', 'नहीं थे', 'जीता', \"नहीं करेगा\", 'होगा', \"नहीं होगा\", 'और', 'का', 'के', 'की', 'है', 'हूँ', 'तो', 'यह', 'जो', 'भी', 'में', 'पर', '।',\n",
        "    'था', 'थे', 'वह', 'वो', 'इस', 'नहीं', 'कि', 'जो', 'के', 'लिए', 'को', 'कर', 'करने', 'तक', \"के\", \"की\", \"से\", \"है\", \"में\", \"हो\", \"को\", \"इस\", \"कर\", \"या\", \"हैं\"\n",
        "])\n",
        "\n",
        "\n",
        "#  Preprocessing\n",
        "def clean_text(text):  # defines a function called clean_text that takes a parameter text--(function and parameter)\n",
        "    text = str(text).lower()  # Converts the input to a string (in case it’s not), and then changes all letters to lowercase\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Removes all punctuation or special characters using a regular expression, keeping only letters, numbers, and whitespace\n",
        "    return text  # It return the cleaned text\n",
        "\n",
        "\n",
        "def preprocess_text(text):  # Defines a function named preprocess_text that takes one input: text\n",
        "    text = clean_text(text)  # it cleans the text by converting it to lowercase and removing special characters (using the clean_text function you saw earlier)\n",
        "    tokens = indic_tokenize.trivial_tokenize(text,\n",
        "                                             lang='hi')  # it tokenizes the text into words (splits it) using a simple tokenizer for Hindi (trivial_tokenize from indic_tokenize).\n",
        "    tokens = [t for t in tokens if\n",
        "              t not in stop_words]  # Filters out stop words (common words like \"है\", \"मैं\", etc., which don’t carry much meaning) from the token list.\n",
        "    return ' '.join(tokens)  # Joins the remaining tokens (words) back into a single string with spaces and returns it\n",
        "\n",
        "\n",
        "data['text'] = data['text'].apply(preprocess_text)  # Cleans and processes all text data in the 'text' column of the dataset.\n",
        "\n",
        "\n",
        "#  Class Balancing\n",
        "# This function balances the dataset by upsampling smaller emotion classes so that all classes have the same number of samples\n",
        "def balance_data(df):  # Defines a function named balance_data that takes a DataFrame df as input\n",
        "    classes = df['emotion'].value_counts().index  # Gets a list of all unique emotion classes in the dataset\n",
        "    max_size = df['emotion'].value_counts().max()  # Finds the count of the most frequent emotion class (the largest class size).\n",
        "    lst = [df]  # Creates a list with the original DataFrame in it, which will later be added to\n",
        "    for cls in classes:  # Starts a loop through each emotion class.\n",
        "        df_class = df[df['emotion'] == cls]  # Selects all rows from the DataFrame that belong to the current emotion class\n",
        "        if len(df_class) < max_size:  # Checks if the current class has fewer samples than the most frequent one\n",
        "            lst.append(resample(df_class, replace=True, n_samples=max_size - len(df_class),\n",
        "                               random_state=42))  # If yes, it upsamples this class (randomly duplicates samples) to match the size of the largest class\n",
        "    return pd.concat(lst)  # Combines the original data with the newly upsampled data and returns the balanced dataset\n",
        "\n",
        "\n",
        "data = balance_data(data)  # Calls the function on your data and replaces it with the balanced version, where all emotion classes now have equal size.\n",
        "\n",
        "\n",
        "#  Train/Test Split\n",
        "# This code splits the dataset into training and testing sets (80%-20%) while keeping the emotion class distribution balanced in both sets.\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2,\n",
        "                              random_state=42)  # Creates a Stratified Shuffle Split object that will split the data once (n_splits=1) into 80% training and 20% testing, while keeping the proportion of each emotion class the same in both sets\n",
        "for train_index, test_index in sss.split(data['text'],\n",
        "                                        data['emotion']):  # Performs the split. it returns the indices for training and testing, ensuring that the emotion labels are evenly distributed\n",
        "    X_train, X_test = data['text'].iloc[\n",
        "        train_index], data['text'].iloc[\n",
        "        test_index]  # Uses the indices to get the training and testing text data\n",
        "    y_train, y_test = data['emotion'].iloc[\n",
        "        train_index], data['emotion'].iloc[\n",
        "        test_index]  # Uses the same indices to get the corresponding emotion labels for training and testing\n",
        "\n",
        "\n",
        "#  Objective Function (alpha, max_feat, ngram_upper) This function trains a Naive Bayes model with given parameters and returns the negative accuracy, so that an optimizer can find the best parameters by minimizing the output\n",
        "def objective_function(params):  # Defines a function called objective_function that takes a list or tuple of parameters: params\n",
        "    alpha, max_feat, ngram_upper = params  # Unpacks the parameters into three variables: alpha (smoothing for Naive Bayes), max_feat (number of max features for TF-IDF), ngram_upper (upper limit for n-grams).\n",
        "    vectorizer = TfidfVectorizer(max_features=int(max_feat),\n",
        "                                 ngram_range=(1, int(ngram_upper)))  # Creates a TF-IDF vectorizer using the given max_feat and ngram_upper\n",
        "    X_train_tfidf = vectorizer.fit_transform(\n",
        "        X_train)  # Fits the TF-IDF vectorizer on the training data and transforms it into a TF-IDF matrix\n",
        "    X_test_tfidf = vectorizer.transform(\n",
        "        X_test)  # Transforms the test data using the same TF-IDF vectorizer (without refitting)\n",
        "    clf = MultinomialNB(\n",
        "        alpha=alpha)  # Creates a Multinomial Naive Bayes classifier with the given alpha value\n",
        "    clf.fit(X_train_tfidf, y_train)  # trains the classifier on the training TF-IDF data\n",
        "    y_pred = clf.predict(\n",
        "        X_test_tfidf)  # Predicts emotion labels for the test TF-IDF data\n",
        "    return -accuracy_score(y_test,\n",
        "                             y_pred)  # Returns the negative accuracy (so that optimization algorithms can minimize this value — minimizing negative accuracy is the same as maximizing accuracy)\n",
        "\n",
        "\n",
        "#  PSO\n",
        "class PSO:\n",
        "    def __init__(self, num_particles, num_iterations,\n",
        "                 bounds):  # Initializes the PSO with the number of particles, number of iterations, and search space bounds\n",
        "        self.num_particles = num_particles  # Stores how many particles (solutions) will be used in the swarm\n",
        "        self.num_iterations = num_iterations  # Stores how many iterations the algorithm will run\n",
        "        self.bounds = bounds  # Stores the bounds for each parameter (as a list of tuples, e.g., [(0.1, 1), (100, 1000)])\n",
        "        self.particles = np.random.uniform([b[0] for b in bounds], [b[1] for b in bounds],\n",
        "                                           size=(num_particles,\n",
        "                                                 len(bounds)))  # Initializes each particle's position randomly within the given bounds. Each particle represents a potential solution\n",
        "        self.velocities = np.zeros_like(\n",
        "            self.particles)  # Initializes the velocity of each particle as a zero vector (same shape as particles)\n",
        "        self.best_positions = self.particles.copy()  # Stores the best position found so far by each particle (initially, it's just the starting position)\n",
        "        self.best_fitness = np.inf * np.ones(\n",
        "            num_particles)  # Initializes the best fitness (error) of each particle to infinity, meaning no good solution has been found yet\n",
        "        self.accuracy = 0.0  # Initialize accuracy\n",
        "\n",
        "    def optimize(self):\n",
        "        for _ in range(self.num_iterations):  # Repeats the optimization process for a fixed number of iterations\n",
        "            for i in range(self.num_particles):  # Loops through each particle in the swarm\n",
        "                fitness = objective_function(\n",
        "                    self.particles[i])  # Evaluates the current particle’s position (solution) using the objective_function\n",
        "                if fitness < self.best_fitness[i]:  # If the current fitness is better than the particle's best so far\n",
        "                    self.best_fitness[i] = fitness\n",
        "                    self.best_positions[i] = self.particles[\n",
        "                        i]  # Update that particle’s best score and best-known position.\n",
        "                g_best = self.best_positions[\n",
        "                    np.argmin(self.best_fitness)]  # Finds the global best position from all particles (the one with the lowest fitness score)\n",
        "                self.velocities[i] += 0.5 * (self.best_positions[i] - self.particles[i]) + 0.5 * (\n",
        "                            g_best - self.particles[i])  # Updates the particle’s velocity based on: Its own best-known position, The swarm's global best position with weights of 0.5 (you can think of these as how much each influence the movement).\n",
        "                self.particles[i] += self.velocities[\n",
        "                    i]  # Moves the particle based on the new velocity\n",
        "                self.particles[i] = np.clip(self.particles[i], [b[0] for b in self.bounds],\n",
        "                                           [b[1] for b in\n",
        "                                            self.bounds])  # Ensures the new position stays within the allowed bounds\n",
        "        best_params = self.best_positions[np.argmin(self.best_fitness)]\n",
        "        vectorizer = TfidfVectorizer(max_features=int(best_params[1]),\n",
        "                                     ngram_range=(1, int(best_params[2])))\n",
        "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "        X_test_tfidf = vectorizer.transform(X_test)\n",
        "        clf = MultinomialNB(alpha=best_params[0])\n",
        "        clf.fit(X_train_tfidf, y_train)\n",
        "        y_pred = clf.predict(X_test_tfidf)\n",
        "        self.accuracy = accuracy_score(y_test, y_pred)\n",
        "        return best_params  # After all iterations, returns the best solution found by the swarm..\n",
        "\n",
        "\n",
        "#  Firefly\n",
        "class Firefly:\n",
        "    def __init__(self, num_fireflies, num_iterations,\n",
        "                 bounds):  # Initializes the Firefly algorithm with : num_fireflies- how many fireflies (solutions) to use, num_iterations: how many times the algorithm will update, bounds: the allowed range for each parameter.\n",
        "        self.num_fireflies = num_fireflies\n",
        "        self.num_iterations = num_iterations\n",
        "        self.bounds = bounds  # Stores the inputs as attributes of the class\n",
        "        self.fireflies = np.random.uniform([b[0] for b in bounds], [b[1] for b in bounds],\n",
        "                                           size=(num_fireflies,\n",
        "                                                 len(bounds)))  # Initializes the positions of all fireflies randomly within the given bounds. Each firefly is a candidate solution\n",
        "        self.light_intensities = np.inf * np.ones(\n",
        "            num_fireflies)  # Initializes each firefly’s light intensity (fitness value) to infinity, meaning no good solution has been found yet\n",
        "        self.accuracy = 0.0  # Initialize accuracy\n",
        "\n",
        "    def optimize(self):\n",
        "        for _ in range(self.num_iterations):  # Loops for a fixed number of iterations to update fireflies’ positions\n",
        "            for i in range(self.num_fireflies):  # Loops over each firefly i\n",
        "                fitness = objective_function(\n",
        "                    self.fireflies[i])  # Calculates the fitness (or brightness) of firefly i\n",
        "                if fitness < self.light_intensities[i]:\n",
        "                    self.light_intensities[i] = fitness  # If this new fitness is better (lower), update the firefly’s brightness\n",
        "                for j in range(self.num_fireflies):  # Now compare this firefly i to all others j\n",
        "                    if self.light_intensities[j] < self.light_intensities[i]:  # If firefly j is brighter (better solution) than firefly i\n",
        "                        self.fireflies[i] += 0.2 * (self.fireflies[j] - self.fireflies[i]) + 0.2 * np.random.uniform(-1, 1,\n",
        "                                                                                                                   size=len(\n",
        "                                                                                                                       self.bounds))  # Firefly i moves toward firefly j, plus a little random movement (to explore new areas)\n",
        "                        self.fireflies[i] = np.clip(self.fireflies[i], [b[0] for b in self.bounds],\n",
        "                                                   [b[1] for b in\n",
        "                                                    self.bounds])  # Keeps firefly i's new position within the allowed bounds\n",
        "        best_params = self.fireflies[np.argmin(self.light_intensities)]\n",
        "        vectorizer = TfidfVectorizer(max_features=int(best_params[1]),\n",
        "                                     ngram_range=(1, int(best_params[2])))\n",
        "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "        X_test_tfidf = vectorizer.transform(X_test)\n",
        "        clf = MultinomialNB(alpha=best_params[0])\n",
        "        clf.fit(X_train_tfidf, y_train)\n",
        "        y_pred = clf.predict(X_test_tfidf)\n",
        "        self.accuracy = accuracy_score(y_test, y_pred)\n",
        "        return best_params  # After all iterations, returns the best firefly (the one with the lowest fitness value)\n",
        "\n",
        "\n",
        "# 🥚 Cuckoo Search\n",
        "class Cuckoo:\n",
        "    def __init__(self, num_cuckoos, num_iterations,\n",
        "                 bounds):  # Initializes the Cuckoo Search with: num_cuckoos - number of cuckoo birds (candidate solutions),num_iterations: how many times the algorithm will run, bounds: the allowed range for each parameter.\n",
        "        self.num_cuckoos = num_cuckoos\n",
        "        self.num_iterations = num_iterations\n",
        "        self.bounds = bounds  # Stores the given inputs as class attributes\n",
        "        self.cuckoos = np.random.uniform([b[0] for b in bounds], [b[1] for b in bounds],\n",
        "                                         size=(num_cuckoos,\n",
        "                                               len(bounds)))  # Randomly initializes the position of each cuckoo within the specified bounds. Each cuckoo represents a potential solution\n",
        "        self.fitness = np.inf * np.ones(\n",
        "            num_cuckoos)  # Initializes the fitness (error score) for each cuckoo as infinity, meaning no good solution is known yet\n",
        "        self.accuracy = 0.0  # Initialize accuracy\n",
        "\n",
        "    def optimize(self):\n",
        "        for _ in range(self.num_iterations):  # Repeats the search process for a set number of iterations\n",
        "            for i in range(self.num_cuckoos):  # Loops over each cuckoo (candidate solution)\n",
        "                fitness = objective_function(\n",
        "                    self.cuckoos[i])  # Calculates how good the current cuckoo’s solution is (lower is better)\n",
        "                if fitness < self.fitness[i]:\n",
        "                    self.fitness[i] = fitness  # If this new fitness is better than what was recorded before, update it\n",
        "                j = np.random.randint(0, self.num_cuckoos)  # Randomly pick another cuckoo j\n",
        "                if self.fitness[j] < self.fitness[i]:  # If cuckoo j has a better solution (lower fitness) than cuckoo i\n",
        "                    self.cuckoos[i] += 0.2 * (self.cuckoos[j] - self.cuckoos[i]) + 0.2 * np.random.uniform(-1, 1,\n",
        "                                                                                                               size=len(\n",
        "                                                                                                                   self.bounds))  # Move cuckoo i a bit toward cuckoo j, and add a little randomness to explore\n",
        "                    self.cuckoos[i] = np.clip(self.cuckoos[i], [b[0] for b in self.bounds],\n",
        "                                             [b[1] for b in\n",
        "                                              self.bounds])  # Make sure the new position stays within the allowed parameter range\n",
        "        best_params = self.cuckoos[np.argmin(self.fitness)]\n",
        "        vectorizer = TfidfVectorizer(max_features=int(best_params[1]),\n",
        "                                     ngram_range=(1, int(best_params[2])))\n",
        "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "        X_test_tfidf = vectorizer.transform(X_test)\n",
        "        clf = MultinomialNB(alpha=best_params[0])\n",
        "        clf.fit(X_train_tfidf, y_train)\n",
        "        y_pred = clf.predict(X_test_tfidf)\n",
        "        self.accuracy = accuracy_score(y_test, y_pred)\n",
        "        return best_params  # After all iterations, return the best solution found (the cuckoo with the lowest fitness)\n",
        "\n",
        "\n",
        "#  Hybrid Optimization\n",
        "class HybridOptimization:\n",
        "    def __init__(self, num_particles, num_fireflies, num_cuckoos, num_iterations, bounds):\n",
        "        self.pso = PSO(num_particles, num_iterations, bounds)\n",
        "        self.fa = Firefly(num_fireflies, num_iterations, bounds)\n",
        "        self.cs = Cuckoo(num_cuckoos, num_iterations,\n",
        "                         bounds)  # Initializes each algorithm with the same number of iterations and parameter bounds\n",
        "\n",
        "    def optimize(self):  # Starts the hybrid optimization process\n",
        "        pso_best = self.pso.optimize()\n",
        "        fa_best = self.fa.optimize()\n",
        "        cs_best = self.cs.optimize()  # Runs each algorithms separately and saves their best result.\n",
        "        best_fitness = np.inf\n",
        "        best_position = None  # Initialize variable to track the overall best solution\n",
        "        best_accuracy = 0.0\n",
        "        pso_accuracy = self.pso.accuracy\n",
        "        fa_accuracy = self.fa.accuracy\n",
        "        cs_accuracy = self.cs.accuracy\n",
        "\n",
        "        for i, position in enumerate([pso_best, fa_best, cs_best]):\n",
        "            fitness = objective_function(position)\n",
        "            if fitness < best_fitness:\n",
        "                best_fitness = fitness\n",
        "                best_position = position\n",
        "                if i == 0:\n",
        "                    best_accuracy = pso_accuracy\n",
        "                elif i == 1:\n",
        "                    best_accuracy = fa_accuracy\n",
        "                else:\n",
        "                    best_accuracy = cs_accuracy\n",
        "                    # Compares the three result and keep the one with hte lowest fitness (best performance)\n",
        "        print(f\"PSO Accuracy: {pso_accuracy:.4f}\")\n",
        "        print(f\"Firefly Accuracy: {fa_accuracy:.4f}\")\n",
        "        print(f\"Cuckoo Accuracy: {cs_accuracy:.4f}\")\n",
        "        return best_position  # Returns the best solution found among the three algorithms\n",
        "\n",
        "\n",
        "#  Run Optimization\n",
        "bounds = [(0.001, 1.0), (1000, 8000),\n",
        "          (1, 2)]  # alpha  from 0.001 to 1.0 (used in MultinomialNB), max_features from 1000 to 8000 (for TF-IDF vectorizer), ngram_uppe reither 1 or 2 (for unigrams or bigrams)\n",
        "\"\"\"Creates an object of the HybridOptimization class with:\n",
        "5 particles for PSO,\n",
        "5 fireflies for Firefly Algorithm,\n",
        "5 cuckoos for Cuckoo Search,\n",
        "30 iterations,\n",
        "The parameter bounds defined above.\"\"\"\n",
        "optimizer = HybridOptimization(5, 5, 5, 30, bounds)\n",
        "best_position = optimizer.optimize()  # Runs the hybrid optimization process and gets the best set of parameters found\n",
        "best_alpha, best_max_feat, best_ngram_upper = best_position  # Splits the best solution into individual variables\n",
        "print(\"🔧 Best Parameters:\")\n",
        "print(f\"  Alpha: {best_alpha:.4f}, Max Features: {int(best_max_feat)}, N-gram Upper: {int(best_ngram_upper)}\")\n",
        "\n",
        "#  Final Model with Best Params\n",
        "# This code builds and trains the final emotion classifier using the best hyperparameters found by your hybrid optimize\n",
        "vectorizer = TfidfVectorizer(max_features=int(best_max_feat),\n",
        "                             ngram_range=(1, int(best_ngram_upper)))\n",
        "X_train_tfidf = vectorizer.fit_transform(\n",
        "    X_train)  # Learns the vocabulary from the training text and converts X_train into a TF-IDF matrix\n",
        "X_test_tfidf = vectorizer.transform(\n",
        "    X_test)  # Converts X_test into a TF-IDF matrix using the same vocabulary as training\n",
        "\n",
        "final_clf = MultinomialNB(alpha=best_alpha)  # Creates a Multinomial Naive Bayes classifier using the optimized alpha\n",
        "final_clf.fit(X_train_tfidf, y_train)  # Trains the classifier on the TF-IDF vectors and their corresponding emotions\n",
        "y_pred = final_clf.predict(\n",
        "    X_test_tfidf)  # Uses the trained classifier to predict emotions for the test data\n",
        "\n",
        "#  Evaluation\n",
        "print(\" Final Accuracy:\", accuracy_score(y_test, y_pred))  # Prints the overall accuracy of your final model — how many predictions were correct out of all test samples\n",
        "\"\"\"Prints a detailed classification report, which includes:\n",
        "Precision (how many predicted emotions were correct),\n",
        "Recall (how many actual emotions were correctly found),\n",
        "F1-score (harmonic mean of precision and recall),\n",
        "Support (number of true samples for each class).\"\"\"\n",
        "print(\"\\n Classification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: indic-nlp-library in /usr/local/lib/python3.11/dist-packages (0.92)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: sphinx-argparse in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (0.5.2)\n",
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (3.0.2)\n",
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (2.0.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: sphinx>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from sphinx-argparse->indic-nlp-library) (8.2.3)\n",
            "Requirement already satisfied: docutils>=0.19 in /usr/local/lib/python3.11/dist-packages (from sphinx-argparse->indic-nlp-library) (0.21.2)\n",
            "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in /usr/local/lib/python3.11/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (4.1)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.6)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.18.0)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.3)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.0)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (24.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2025.1.31)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PSO Accuracy: 0.7684\n",
            "Firefly Accuracy: 0.7368\n",
            "Cuckoo Accuracy: 0.7895\n",
            "🔧 Best Parameters:\n",
            "  Alpha: 0.2985, Max Features: 4579, N-gram Upper: 2\n",
            " Final Accuracy: 0.7894736842105263\n",
            "\n",
            " Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       angry       0.50      0.20      0.29         5\n",
            "  confidence       1.00      1.00      1.00         5\n",
            "    disguest       1.00      1.00      1.00         5\n",
            "        envy       0.80      0.80      0.80         5\n",
            "        fear       0.60      0.60      0.60         5\n",
            "   gratitude       1.00      1.00      1.00         5\n",
            "       happy       0.43      0.60      0.50         5\n",
            "        hope       0.83      1.00      0.91         5\n",
            "    jealousy       0.67      0.80      0.73         5\n",
            "  loneliness       1.00      0.80      0.89         5\n",
            "        love       1.00      1.00      1.00         5\n",
            " nervousness       0.80      0.80      0.80         5\n",
            "       pride       0.67      0.40      0.50         5\n",
            "      regret       1.00      0.80      0.89         5\n",
            "      relief       0.62      1.00      0.77         5\n",
            "         sad       0.33      0.20      0.25         5\n",
            "satisfaction       1.00      1.00      1.00         5\n",
            "       shame       0.71      1.00      0.83         5\n",
            "    surprise       1.00      1.00      1.00         5\n",
            "\n",
            "    accuracy                           0.79        95\n",
            "   macro avg       0.79      0.79      0.78        95\n",
            "weighted avg       0.79      0.79      0.78        95\n",
            "\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2A44dB2_maI4",
        "outputId": "7873069c-236e-4a0f-d391-d1c34463802a"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}