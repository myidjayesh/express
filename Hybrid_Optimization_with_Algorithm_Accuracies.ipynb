{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myidjayesh/express/blob/master/Hybrid_Optimization_with_Algorithm_Accuracies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Install Required Packages\n",
        "!pip install nltk scikit-learn pandas indic-nlp-library\n",
        "\n",
        "#  Imports\n",
        "import numpy as np  # math function\n",
        "import pandas as pd  # table format\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Text to binary number\n",
        "from sklearn.model_selection import StratifiedShuffleSplit  # split into training and testing\n",
        "from sklearn.metrics import accuracy_score, classification_report  #\n",
        "from sklearn.naive_bayes import MultinomialNB  # EMotion Guessing algo\n",
        "from sklearn.utils import resample\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "\n",
        "#  Load Dataset\n",
        "data = pd.read_csv('hindi_emotion_dataset261.csv')\n",
        "\n",
        "#  Filter classes with <2 samples\n",
        "emotion_counts = data['emotion'].value_counts()  # Count emotion in dataset\n",
        "valid_classes = emotion_counts[emotion_counts >= 2].index  #\n",
        "data = data[data['emotion'].isin(valid_classes)]  # filters the dataset to keep only those emotion classes that appear at least twice, removing rare classes that occur only once.\n",
        "\n",
        "#  Hindi Stopwords\n",
        "stop_words = set([\n",
        "    '‡§ï‡•á', '‡§ï‡§æ', '‡§ï‡•Ä', '‡§∏‡•á', '‡§ï‡•ã', '‡§Æ‡•á‡§Ç', '‡§™‡§∞', '‡§î‡§∞', '‡§≠‡•Ä', '‡§π‡•à', '‡§Ø‡§π', '‡§•‡•á', '‡§•‡§æ', '‡§π‡•Ç‡§Ç', '‡§π‡•ã', '‡§ó‡§Ø‡§æ', '‡§∞‡§π‡§æ', '‡§è‡§ï', '‡§§‡•ã', '‡§®‡•Ä‡§ö‡•á', '‡§Æ‡•à‡§Ç', '‡§Æ‡•Å‡§ù‡•á', '‡§Æ‡•á‡§∞‡§æ', '‡§∏‡•ç‡§µ‡§Ø‡§Ç', '‡§π‡§Æ', '‡§π‡§Æ‡§æ‡§∞‡§æ', '‡§π‡§Æ‡§æ‡§∞‡§æ ', '‡§µ‡§π', '‡§â‡§∏‡§ï‡§æ', '‡§â‡§∏‡§ï‡§æ', '‡§∏‡•ç‡§µ‡§Ø‡§Ç', '‡§µ‡§π', '‡§â‡§∏‡§ï‡•Ä', '‡§â‡§∏‡§ï‡§æ', '‡§∏‡•ç‡§µ‡§Ø‡§Ç ', '‡§Ø‡§π', \"‡§Ø‡§π ‡§π‡•à\", '‡§á‡§∏‡§ï‡§æ', '‡§∏‡•ç‡§µ‡§Ø‡§Ç', '‡§µ‡•á', '‡§â‡§®‡•ç‡§π‡•á‡§Ç', '‡§â‡§®‡§ï‡§æ', '‡§â‡§®‡§ï‡§æ',\n",
        "    '‡§∏‡•ç‡§µ‡§Ø‡§Ç', '‡§ï‡•ç‡§Ø‡§æ', '‡§ú‡•ã', '‡§ï‡•å‡§®', '‡§ú‡§ø‡§∏‡•á', '‡§Ø‡§π', '‡§µ‡§π', '‡§Ø‡•á', '‡§µ‡•á', '‡§π‡•à‡§Ç', '‡§π‡•à‡§Ç', '‡§≤‡§ø‡§Ø‡§æ', '‡§•‡§æ', '‡§•‡•á', '‡§π‡•ã‡§®‡§æ', '‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ‡§®‡§æ', '‡§π‡•à', '‡§π‡•à', '‡§•‡§æ', '‡§π‡•ã‡§®‡•á', '‡§ï‡§∞‡§®‡§æ', '‡§ï‡§∞‡§§‡§æ ‡§π‡•à', '‡§ï‡§ø‡§Ø‡§æ', '‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à', '!',\n",
        "    '‡§è‡§ï', '‡§è‡§ï', '‡§µ‡§π', '‡§î‡§∞', '‡§≤‡•á‡§ï‡§ø‡§®', '‡§Ö‡§ó‡§∞', '‡§Ø‡§æ', '‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø', '‡§ú‡•à‡§∏‡§æ', '‡§ú‡§¨ ‡§§‡§ï', '‡§ú‡§¨‡§ï‡§ø', '‡§ï‡§æ', '‡§™‡§∞', '‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ', '‡§ï‡•á ‡§≤‡§ø‡§è', '‡§ï‡•á ‡§∏‡§æ‡§•', '‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç', '‡§ñ‡§ø‡§≤‡§æ‡§´', '‡§¨‡•Ä‡§ö ‡§Æ‡•á‡§Ç', '‡§Æ‡•á‡§Ç', '‡§ï‡•á ‡§Æ‡§æ‡§ß‡•ç‡§Ø‡§Æ ‡§∏‡•á', '‡§¶‡•å‡§∞‡§æ‡§®', '‡§™‡§π‡§≤‡•á', '‡§¨‡§æ‡§¶ ‡§Æ‡•á‡§Ç', '‡§ä‡§™‡§∞', '‡§®‡•Ä‡§ö‡•á', '‡§ï‡•ã',\n",
        "    '‡§∏‡•á', '‡§ä‡§™‡§∞', '‡§®‡•Ä‡§ö‡•á', '‡§Ö‡§Ç‡§¶‡§∞', '‡§¨‡§æ‡§π‡§∞', '‡§ö‡§æ‡§≤‡•Ç', '‡§¨‡§Ç‡§¶', '‡§ä‡§™‡§∞', '‡§®‡•Ä‡§ö‡•á', '‡§´‡§ø‡§∞', '‡§Ü‡§ó‡•á', '‡§´‡§ø‡§∞', '‡§è‡§ï ‡§¨‡§æ‡§∞', '‡§Ø‡§π‡§æ‡§Å', '‡§µ‡§π‡§æ‡§Å', '‡§ï‡§¨', '‡§ï‡§π‡§æ‡§Å', '‡§ï‡•ç‡§Ø‡•ã‡§Ç', '‡§ï‡•à‡§∏‡•á', '‡§∏‡§≠‡•Ä', '‡§ï‡•ã‡§à ‡§≠‡•Ä', '‡§¶‡•ã‡§®‡•ã‡§Ç', '‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•á‡§ï', '‡§ï‡•Å‡§õ', '‡§Ö‡§ß‡§ø‡§ï', '‡§Ö‡§ß‡§ø‡§ï‡§§‡§∞',\n",
        "    '‡§Ö‡§®‡•ç‡§Ø', '‡§ï‡•Å‡§õ', '‡§ê‡§∏‡§æ', '‡§®‡§π‡•Ä‡§Ç', '‡§® ‡§π‡•Ä', '‡§®‡§π‡•Ä‡§Ç', '‡§ï‡•á‡§µ‡§≤', '‡§∏‡•ç‡§µ‡§Ø‡§Ç ‡§ï‡§æ', '‡§µ‡§π‡•Ä', '‡§á‡§∏‡§≤‡§ø‡§è', '‡§∏‡•á', '‡§≠‡•Ä', '‡§¨‡§π‡•Å‡§§', '‡§π‡•Ç‡§Ç', '‡§è‡§∏', '‡§ü‡•Ä', '‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç', '‡§ï‡§∞‡•á‡§Ç‡§ó‡•á', '‡§¨‡§∏', '‡§∞‡§π‡§æ', '‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞‡•á‡§Ç', '‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è', '‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è ‡§•‡§æ', '‡§Ö‡§¨', '‡§°‡•Ä', '‡§ï‡§∞‡•Ç‡§Ç‡§ó‡§æ', '‡§è‡§Æ', '‡§ì', '‡§∞‡•Ä', '‡§µ‡•á', '‡§µ‡§æ‡§à', '‡§ê‡§®', '‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡§Ç', '‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞ ‡§∏‡§ï‡•á', '‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞ ‡§∏‡§ï‡•á', '‡§®‡§π‡•Ä‡§Ç ‡§ï‡§ø‡§Ø‡§æ', '‡§®‡§π‡•Ä‡§Ç ‡§ï‡§ø‡§Ø‡§æ', '‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞‡§§‡§æ', '‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞‡§§‡§æ', '‡§®‡§π‡•Ä‡§Ç ‡§•‡§æ', \"‡§®‡§π‡•Ä‡§Ç ‡§π‡•à\", '‡§Æ‡§æ‡§Å', '‡§π‡•ã ‡§∏‡§ï‡§§‡§æ ‡§π‡•à',\n",
        "    \"‡§π‡•ã ‡§∏‡§ï‡§§‡§æ ‡§π‡•à ‡§®‡§π‡•Ä‡§Ç\", '‡§ö‡§æ‡§π‡§ø‡§è', \"‡§®‡§π‡•Ä‡§Ç ‡§ö‡§æ‡§π‡§ø‡§è\", '‡§ú‡§º‡§∞‡•Ç‡§∞‡§§ ‡§®‡§π‡•Ä‡§Ç', \"‡§ú‡§º‡§∞‡•Ç‡§∞‡§§ ‡§®‡§π‡•Ä‡§Ç\", '‡§∂‡§æ‡§®', \"‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è\", '‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è', '‡§®‡§π‡•Ä‡§Ç ‡§•‡§æ', '‡§®‡§π‡•Ä‡§Ç ‡§•‡§æ', '‡§®‡§π‡•Ä‡§Ç ‡§•‡•á', '‡§®‡§π‡•Ä‡§Ç ‡§•‡•á', '‡§ú‡•Ä‡§§‡§æ', \"‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞‡•á‡§ó‡§æ\", '‡§π‡•ã‡§ó‡§æ', \"‡§®‡§π‡•Ä‡§Ç ‡§π‡•ã‡§ó‡§æ\", '‡§î‡§∞', '‡§ï‡§æ', '‡§ï‡•á', '‡§ï‡•Ä', '‡§π‡•à', '‡§π‡•Ç‡§Å', '‡§§‡•ã', '‡§Ø‡§π', '‡§ú‡•ã', '‡§≠‡•Ä', '‡§Æ‡•á‡§Ç', '‡§™‡§∞', '‡•§',\n",
        "    '‡§•‡§æ', '‡§•‡•á', '‡§µ‡§π', '‡§µ‡•ã', '‡§á‡§∏', '‡§®‡§π‡•Ä‡§Ç', '‡§ï‡§ø', '‡§ú‡•ã', '‡§ï‡•á', '‡§≤‡§ø‡§è', '‡§ï‡•ã', '‡§ï‡§∞', '‡§ï‡§∞‡§®‡•á', '‡§§‡§ï', \"‡§ï‡•á\", \"‡§ï‡•Ä\", \"‡§∏‡•á\", \"‡§π‡•à\", \"‡§Æ‡•á‡§Ç\", \"‡§π‡•ã\", \"‡§ï‡•ã\", \"‡§á‡§∏\", \"‡§ï‡§∞\", \"‡§Ø‡§æ\", \"‡§π‡•à‡§Ç\"\n",
        "])\n",
        "\n",
        "\n",
        "#  Preprocessing\n",
        "def clean_text(text):  # defines a function called clean_text that takes a parameter text--(function and parameter)\n",
        "    text = str(text).lower()  # Converts the input to a string (in case it‚Äôs not), and then changes all letters to lowercase\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Removes all punctuation or special characters using a regular expression, keeping only letters, numbers, and whitespace\n",
        "    return text  # It return the cleaned text\n",
        "\n",
        "\n",
        "def preprocess_text(text):  # Defines a function named preprocess_text that takes one input: text\n",
        "    text = clean_text(text)  # it cleans the text by converting it to lowercase and removing special characters (using the clean_text function you saw earlier)\n",
        "    tokens = indic_tokenize.trivial_tokenize(text,\n",
        "                                             lang='hi')  # it tokenizes the text into words (splits it) using a simple tokenizer for Hindi (trivial_tokenize from indic_tokenize).\n",
        "    tokens = [t for t in tokens if\n",
        "              t not in stop_words]  # Filters out stop words (common words like \"‡§π‡•à\", \"‡§Æ‡•à‡§Ç\", etc., which don‚Äôt carry much meaning) from the token list.\n",
        "    return ' '.join(tokens)  # Joins the remaining tokens (words) back into a single string with spaces and returns it\n",
        "\n",
        "\n",
        "data['text'] = data['text'].apply(preprocess_text)  # Cleans and processes all text data in the 'text' column of the dataset.\n",
        "\n",
        "\n",
        "#  Class Balancing\n",
        "# This function balances the dataset by upsampling smaller emotion classes so that all classes have the same number of samples\n",
        "def balance_data(df):  # Defines a function named balance_data that takes a DataFrame df as input\n",
        "    classes = df['emotion'].value_counts().index  # Gets a list of all unique emotion classes in the dataset\n",
        "    max_size = df['emotion'].value_counts().max()  # Finds the count of the most frequent emotion class (the largest class size).\n",
        "    lst = [df]  # Creates a list with the original DataFrame in it, which will later be added to\n",
        "    for cls in classes:  # Starts a loop through each emotion class.\n",
        "        df_class = df[df['emotion'] == cls]  # Selects all rows from the DataFrame that belong to the current emotion class\n",
        "        if len(df_class) < max_size:  # Checks if the current class has fewer samples than the most frequent one\n",
        "            lst.append(resample(df_class, replace=True, n_samples=max_size - len(df_class),\n",
        "                               random_state=42))  # If yes, it upsamples this class (randomly duplicates samples) to match the size of the largest class\n",
        "    return pd.concat(lst)  # Combines the original data with the newly upsampled data and returns the balanced dataset\n",
        "\n",
        "\n",
        "data = balance_data(data)  # Calls the function on your data and replaces it with the balanced version, where all emotion classes now have equal size.\n",
        "\n",
        "\n",
        "#  Train/Test Split\n",
        "# This code splits the dataset into training and testing sets (80%-20%) while keeping the emotion class distribution balanced in both sets.\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2,\n",
        "                              random_state=42)  # Creates a Stratified Shuffle Split object that will split the data once (n_splits=1) into 80% training and 20% testing, while keeping the proportion of each emotion class the same in both sets\n",
        "for train_index, test_index in sss.split(data['text'],\n",
        "                                        data['emotion']):  # Performs the split. it returns the indices for training and testing, ensuring that the emotion labels are evenly distributed\n",
        "    X_train, X_test = data['text'].iloc[\n",
        "        train_index], data['text'].iloc[\n",
        "        test_index]  # Uses the indices to get the training and testing text data\n",
        "    y_train, y_test = data['emotion'].iloc[\n",
        "        train_index], data['emotion'].iloc[\n",
        "        test_index]  # Uses the same indices to get the corresponding emotion labels for training and testing\n",
        "\n",
        "\n",
        "#  Objective Function (alpha, max_feat, ngram_upper) This function trains a Naive Bayes model with given parameters and returns the negative accuracy, so that an optimizer can find the best parameters by minimizing the output\n",
        "def objective_function(params):  # Defines a function called objective_function that takes a list or tuple of parameters: params\n",
        "    alpha, max_feat, ngram_upper = params  # Unpacks the parameters into three variables: alpha (smoothing for Naive Bayes), max_feat (number of max features for TF-IDF), ngram_upper (upper limit for n-grams).\n",
        "    vectorizer = TfidfVectorizer(max_features=int(max_feat),\n",
        "                                 ngram_range=(1, int(ngram_upper)))  # Creates a TF-IDF vectorizer using the given max_feat and ngram_upper\n",
        "    X_train_tfidf = vectorizer.fit_transform(\n",
        "        X_train)  # Fits the TF-IDF vectorizer on the training data and transforms it into a TF-IDF matrix\n",
        "    X_test_tfidf = vectorizer.transform(\n",
        "        X_test)  # Transforms the test data using the same TF-IDF vectorizer (without refitting)\n",
        "    clf = MultinomialNB(\n",
        "        alpha=alpha)  # Creates a Multinomial Naive Bayes classifier with the given alpha value\n",
        "    clf.fit(X_train_tfidf, y_train)  # trains the classifier on the training TF-IDF data\n",
        "    y_pred = clf.predict(\n",
        "        X_test_tfidf)  # Predicts emotion labels for the test TF-IDF data\n",
        "    return -accuracy_score(y_test,\n",
        "                             y_pred)  # Returns the negative accuracy (so that optimization algorithms can minimize this value ‚Äî minimizing negative accuracy is the same as maximizing accuracy)\n",
        "\n",
        "\n",
        "#  PSO\n",
        "class PSO:\n",
        "    def __init__(self, num_particles, num_iterations,\n",
        "                 bounds):  # Initializes the PSO with the number of particles, number of iterations, and search space bounds\n",
        "        self.num_particles = num_particles  # Stores how many particles (solutions) will be used in the swarm\n",
        "        self.num_iterations = num_iterations  # Stores how many iterations the algorithm will run\n",
        "        self.bounds = bounds  # Stores the bounds for each parameter (as a list of tuples, e.g., [(0.1, 1), (100, 1000)])\n",
        "        self.particles = np.random.uniform([b[0] for b in bounds], [b[1] for b in bounds],\n",
        "                                           size=(num_particles,\n",
        "                                                 len(bounds)))  # Initializes each particle's position randomly within the given bounds. Each particle represents a potential solution\n",
        "        self.velocities = np.zeros_like(\n",
        "            self.particles)  # Initializes the velocity of each particle as a zero vector (same shape as particles)\n",
        "        self.best_positions = self.particles.copy()  # Stores the best position found so far by each particle (initially, it's just the starting position)\n",
        "        self.best_fitness = np.inf * np.ones(\n",
        "            num_particles)  # Initializes the best fitness (error) of each particle to infinity, meaning no good solution has been found yet\n",
        "        self.accuracy = 0.0  # Initialize accuracy\n",
        "\n",
        "    def optimize(self):\n",
        "        for _ in range(self.num_iterations):  # Repeats the optimization process for a fixed number of iterations\n",
        "            for i in range(self.num_particles):  # Loops through each particle in the swarm\n",
        "                fitness = objective_function(\n",
        "                    self.particles[i])  # Evaluates the current particle‚Äôs position (solution) using the objective_function\n",
        "                if fitness < self.best_fitness[i]:  # If the current fitness is better than the particle's best so far\n",
        "                    self.best_fitness[i] = fitness\n",
        "                    self.best_positions[i] = self.particles[\n",
        "                        i]  # Update that particle‚Äôs best score and best-known position.\n",
        "                g_best = self.best_positions[\n",
        "                    np.argmin(self.best_fitness)]  # Finds the global best position from all particles (the one with the lowest fitness score)\n",
        "                self.velocities[i] += 0.5 * (self.best_positions[i] - self.particles[i]) + 0.5 * (\n",
        "                            g_best - self.particles[i])  # Updates the particle‚Äôs velocity based on: Its own best-known position, The swarm's global best position with weights of 0.5 (you can think of these as how much each influence the movement).\n",
        "                self.particles[i] += self.velocities[\n",
        "                    i]  # Moves the particle based on the new velocity\n",
        "                self.particles[i] = np.clip(self.particles[i], [b[0] for b in self.bounds],\n",
        "                                           [b[1] for b in\n",
        "                                            self.bounds])  # Ensures the new position stays within the allowed bounds\n",
        "        best_params = self.best_positions[np.argmin(self.best_fitness)]\n",
        "        vectorizer = TfidfVectorizer(max_features=int(best_params[1]),\n",
        "                                     ngram_range=(1, int(best_params[2])))\n",
        "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "        X_test_tfidf = vectorizer.transform(X_test)\n",
        "        clf = MultinomialNB(alpha=best_params[0])\n",
        "        clf.fit(X_train_tfidf, y_train)\n",
        "        y_pred = clf.predict(X_test_tfidf)\n",
        "        self.accuracy = accuracy_score(y_test, y_pred)\n",
        "        return best_params  # After all iterations, returns the best solution found by the swarm..\n",
        "\n",
        "\n",
        "#  Firefly\n",
        "class Firefly:\n",
        "    def __init__(self, num_fireflies, num_iterations,\n",
        "                 bounds):  # Initializes the Firefly algorithm with : num_fireflies- how many fireflies (solutions) to use, num_iterations: how many times the algorithm will update, bounds: the allowed range for each parameter.\n",
        "        self.num_fireflies = num_fireflies\n",
        "        self.num_iterations = num_iterations\n",
        "        self.bounds = bounds  # Stores the inputs as attributes of the class\n",
        "        self.fireflies = np.random.uniform([b[0] for b in bounds], [b[1] for b in bounds],\n",
        "                                           size=(num_fireflies,\n",
        "                                                 len(bounds)))  # Initializes the positions of all fireflies randomly within the given bounds. Each firefly is a candidate solution\n",
        "        self.light_intensities = np.inf * np.ones(\n",
        "            num_fireflies)  # Initializes each firefly‚Äôs light intensity (fitness value) to infinity, meaning no good solution has been found yet\n",
        "        self.accuracy = 0.0  # Initialize accuracy\n",
        "\n",
        "    def optimize(self):\n",
        "        for _ in range(self.num_iterations):  # Loops for a fixed number of iterations to update fireflies‚Äô positions\n",
        "            for i in range(self.num_fireflies):  # Loops over each firefly i\n",
        "                fitness = objective_function(\n",
        "                    self.fireflies[i])  # Calculates the fitness (or brightness) of firefly i\n",
        "                if fitness < self.light_intensities[i]:\n",
        "                    self.light_intensities[i] = fitness  # If this new fitness is better (lower), update the firefly‚Äôs brightness\n",
        "                for j in range(self.num_fireflies):  # Now compare this firefly i to all others j\n",
        "                    if self.light_intensities[j] < self.light_intensities[i]:  # If firefly j is brighter (better solution) than firefly i\n",
        "                        self.fireflies[i] += 0.2 * (self.fireflies[j] - self.fireflies[i]) + 0.2 * np.random.uniform(-1, 1,\n",
        "                                                                                                                   size=len(\n",
        "                                                                                                                       self.bounds))  # Firefly i moves toward firefly j, plus a little random movement (to explore new areas)\n",
        "                        self.fireflies[i] = np.clip(self.fireflies[i], [b[0] for b in self.bounds],\n",
        "                                                   [b[1] for b in\n",
        "                                                    self.bounds])  # Keeps firefly i's new position within the allowed bounds\n",
        "        best_params = self.fireflies[np.argmin(self.light_intensities)]\n",
        "        vectorizer = TfidfVectorizer(max_features=int(best_params[1]),\n",
        "                                     ngram_range=(1, int(best_params[2])))\n",
        "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "        X_test_tfidf = vectorizer.transform(X_test)\n",
        "        clf = MultinomialNB(alpha=best_params[0])\n",
        "        clf.fit(X_train_tfidf, y_train)\n",
        "        y_pred = clf.predict(X_test_tfidf)\n",
        "        self.accuracy = accuracy_score(y_test, y_pred)\n",
        "        return best_params  # After all iterations, returns the best firefly (the one with the lowest fitness value)\n",
        "\n",
        "\n",
        "# ü•ö Cuckoo Search\n",
        "class Cuckoo:\n",
        "    def __init__(self, num_cuckoos, num_iterations,\n",
        "                 bounds):  # Initializes the Cuckoo Search with: num_cuckoos - number of cuckoo birds (candidate solutions),num_iterations: how many times the algorithm will run, bounds: the allowed range for each parameter.\n",
        "        self.num_cuckoos = num_cuckoos\n",
        "        self.num_iterations = num_iterations\n",
        "        self.bounds = bounds  # Stores the given inputs as class attributes\n",
        "        self.cuckoos = np.random.uniform([b[0] for b in bounds], [b[1] for b in bounds],\n",
        "                                         size=(num_cuckoos,\n",
        "                                               len(bounds)))  # Randomly initializes the position of each cuckoo within the specified bounds. Each cuckoo represents a potential solution\n",
        "        self.fitness = np.inf * np.ones(\n",
        "            num_cuckoos)  # Initializes the fitness (error score) for each cuckoo as infinity, meaning no good solution is known yet\n",
        "        self.accuracy = 0.0  # Initialize accuracy\n",
        "\n",
        "    def optimize(self):\n",
        "        for _ in range(self.num_iterations):  # Repeats the search process for a set number of iterations\n",
        "            for i in range(self.num_cuckoos):  # Loops over each cuckoo (candidate solution)\n",
        "                fitness = objective_function(\n",
        "                    self.cuckoos[i])  # Calculates how good the current cuckoo‚Äôs solution is (lower is better)\n",
        "                if fitness < self.fitness[i]:\n",
        "                    self.fitness[i] = fitness  # If this new fitness is better than what was recorded before, update it\n",
        "                j = np.random.randint(0, self.num_cuckoos)  # Randomly pick another cuckoo j\n",
        "                if self.fitness[j] < self.fitness[i]:  # If cuckoo j has a better solution (lower fitness) than cuckoo i\n",
        "                    self.cuckoos[i] += 0.2 * (self.cuckoos[j] - self.cuckoos[i]) + 0.2 * np.random.uniform(-1, 1,\n",
        "                                                                                                               size=len(\n",
        "                                                                                                                   self.bounds))  # Move cuckoo i a bit toward cuckoo j, and add a little randomness to explore\n",
        "                    self.cuckoos[i] = np.clip(self.cuckoos[i], [b[0] for b in self.bounds],\n",
        "                                             [b[1] for b in\n",
        "                                              self.bounds])  # Make sure the new position stays within the allowed parameter range\n",
        "        best_params = self.cuckoos[np.argmin(self.fitness)]\n",
        "        vectorizer = TfidfVectorizer(max_features=int(best_params[1]),\n",
        "                                     ngram_range=(1, int(best_params[2])))\n",
        "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "        X_test_tfidf = vectorizer.transform(X_test)\n",
        "        clf = MultinomialNB(alpha=best_params[0])\n",
        "        clf.fit(X_train_tfidf, y_train)\n",
        "        y_pred = clf.predict(X_test_tfidf)\n",
        "        self.accuracy = accuracy_score(y_test, y_pred)\n",
        "        return best_params  # After all iterations, return the best solution found (the cuckoo with the lowest fitness)\n",
        "\n",
        "\n",
        "#  Hybrid Optimization\n",
        "class HybridOptimization:\n",
        "    def __init__(self, num_particles, num_fireflies, num_cuckoos, num_iterations, bounds):\n",
        "        self.pso = PSO(num_particles, num_iterations, bounds)\n",
        "        self.fa = Firefly(num_fireflies, num_iterations, bounds)\n",
        "        self.cs = Cuckoo(num_cuckoos, num_iterations,\n",
        "                         bounds)  # Initializes each algorithm with the same number of iterations and parameter bounds\n",
        "\n",
        "    def optimize(self):  # Starts the hybrid optimization process\n",
        "        pso_best = self.pso.optimize()\n",
        "        fa_best = self.fa.optimize()\n",
        "        cs_best = self.cs.optimize()  # Runs each algorithms separately and saves their best result.\n",
        "        best_fitness = np.inf\n",
        "        best_position = None  # Initialize variable to track the overall best solution\n",
        "        best_accuracy = 0.0\n",
        "        pso_accuracy = self.pso.accuracy\n",
        "        fa_accuracy = self.fa.accuracy\n",
        "        cs_accuracy = self.cs.accuracy\n",
        "\n",
        "        for i, position in enumerate([pso_best, fa_best, cs_best]):\n",
        "            fitness = objective_function(position)\n",
        "            if fitness < best_fitness:\n",
        "                best_fitness = fitness\n",
        "                best_position = position\n",
        "                if i == 0:\n",
        "                    best_accuracy = pso_accuracy\n",
        "                elif i == 1:\n",
        "                    best_accuracy = fa_accuracy\n",
        "                else:\n",
        "                    best_accuracy = cs_accuracy\n",
        "                    # Compares the three result and keep the one with hte lowest fitness (best performance)\n",
        "        print(f\"PSO Accuracy: {pso_accuracy:.4f}\")\n",
        "        print(f\"Firefly Accuracy: {fa_accuracy:.4f}\")\n",
        "        print(f\"Cuckoo Accuracy: {cs_accuracy:.4f}\")\n",
        "        return best_position  # Returns the best solution found among the three algorithms\n",
        "\n",
        "\n",
        "#  Run Optimization\n",
        "bounds = [(0.001, 1.0), (1000, 8000),\n",
        "          (1, 2)]  # alpha  from 0.001 to 1.0 (used in MultinomialNB), max_features from 1000 to 8000 (for TF-IDF vectorizer), ngram_uppe reither 1 or 2 (for unigrams or bigrams)\n",
        "\"\"\"Creates an object of the HybridOptimization class with:\n",
        "5 particles for PSO,\n",
        "5 fireflies for Firefly Algorithm,\n",
        "5 cuckoos for Cuckoo Search,\n",
        "30 iterations,\n",
        "The parameter bounds defined above.\"\"\"\n",
        "optimizer = HybridOptimization(5, 5, 5, 30, bounds)\n",
        "best_position = optimizer.optimize()  # Runs the hybrid optimization process and gets the best set of parameters found\n",
        "best_alpha, best_max_feat, best_ngram_upper = best_position  # Splits the best solution into individual variables\n",
        "print(\"üîß Best Parameters:\")\n",
        "print(f\"  Alpha: {best_alpha:.4f}, Max Features: {int(best_max_feat)}, N-gram Upper: {int(best_ngram_upper)}\")\n",
        "\n",
        "#  Final Model with Best Params\n",
        "# This code builds and trains the final emotion classifier using the best hyperparameters found by your hybrid optimize\n",
        "vectorizer = TfidfVectorizer(max_features=int(best_max_feat),\n",
        "                             ngram_range=(1, int(best_ngram_upper)))\n",
        "X_train_tfidf = vectorizer.fit_transform(\n",
        "    X_train)  # Learns the vocabulary from the training text and converts X_train into a TF-IDF matrix\n",
        "X_test_tfidf = vectorizer.transform(\n",
        "    X_test)  # Converts X_test into a TF-IDF matrix using the same vocabulary as training\n",
        "\n",
        "final_clf = MultinomialNB(alpha=best_alpha)  # Creates a Multinomial Naive Bayes classifier using the optimized alpha\n",
        "final_clf.fit(X_train_tfidf, y_train)  # Trains the classifier on the TF-IDF vectors and their corresponding emotions\n",
        "y_pred = final_clf.predict(\n",
        "    X_test_tfidf)  # Uses the trained classifier to predict emotions for the test data\n",
        "\n",
        "#  Evaluation\n",
        "print(\" Final Accuracy:\", accuracy_score(y_test, y_pred))  # Prints the overall accuracy of your final model ‚Äî how many predictions were correct out of all test samples\n",
        "\"\"\"Prints a detailed classification report, which includes:\n",
        "Precision (how many predicted emotions were correct),\n",
        "Recall (how many actual emotions were correctly found),\n",
        "F1-score (harmonic mean of precision and recall),\n",
        "Support (number of true samples for each class).\"\"\"\n",
        "print(\"\\n Classification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: indic-nlp-library in /usr/local/lib/python3.11/dist-packages (0.92)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: sphinx-argparse in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (0.5.2)\n",
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (3.0.2)\n",
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (2.0.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: sphinx>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from sphinx-argparse->indic-nlp-library) (8.2.3)\n",
            "Requirement already satisfied: docutils>=0.19 in /usr/local/lib/python3.11/dist-packages (from sphinx-argparse->indic-nlp-library) (0.21.2)\n",
            "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in /usr/local/lib/python3.11/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (4.1)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.6)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.18.0)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.3)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.0)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (24.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2025.1.31)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PSO Accuracy: 0.7684\n",
            "Firefly Accuracy: 0.7368\n",
            "Cuckoo Accuracy: 0.7895\n",
            "üîß Best Parameters:\n",
            "  Alpha: 0.2985, Max Features: 4579, N-gram Upper: 2\n",
            " Final Accuracy: 0.7894736842105263\n",
            "\n",
            " Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       angry       0.50      0.20      0.29         5\n",
            "  confidence       1.00      1.00      1.00         5\n",
            "    disguest       1.00      1.00      1.00         5\n",
            "        envy       0.80      0.80      0.80         5\n",
            "        fear       0.60      0.60      0.60         5\n",
            "   gratitude       1.00      1.00      1.00         5\n",
            "       happy       0.43      0.60      0.50         5\n",
            "        hope       0.83      1.00      0.91         5\n",
            "    jealousy       0.67      0.80      0.73         5\n",
            "  loneliness       1.00      0.80      0.89         5\n",
            "        love       1.00      1.00      1.00         5\n",
            " nervousness       0.80      0.80      0.80         5\n",
            "       pride       0.67      0.40      0.50         5\n",
            "      regret       1.00      0.80      0.89         5\n",
            "      relief       0.62      1.00      0.77         5\n",
            "         sad       0.33      0.20      0.25         5\n",
            "satisfaction       1.00      1.00      1.00         5\n",
            "       shame       0.71      1.00      0.83         5\n",
            "    surprise       1.00      1.00      1.00         5\n",
            "\n",
            "    accuracy                           0.79        95\n",
            "   macro avg       0.79      0.79      0.78        95\n",
            "weighted avg       0.79      0.79      0.78        95\n",
            "\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2A44dB2_maI4",
        "outputId": "7873069c-236e-4a0f-d391-d1c34463802a"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}